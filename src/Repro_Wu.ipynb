{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import random\n",
    "from Wu_reproduction.HSIC import HSIC2\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "\n",
    "    def feature_map(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def sample_rff_weights(nin, nout, sigma):\n",
    "    return np.random.normal(loc=0, scale=sigma, size=(nin, nout))\n",
    "\n",
    "def sample_rff_b(nin):\n",
    "    return np.random.uniform(0, 2 * np.pi, size=(nin,1))\n",
    "\n",
    "\n",
    "class RBFKernel():\n",
    "    def __init__(self, nin, nout, sigma, static=True):\n",
    "        self.gamma = 1 / (2 * sigma * sigma)\n",
    "        self.rff_weights = sample_rff_weights(nin, nout, sigma)\n",
    "        self.b = sample_rff_b(nin)\n",
    "        self.define_feature_map(static, nout)\n",
    "\n",
    "    def define_feature_map(self, static, nout):\n",
    "        if static:\n",
    "            def feature_map(x):\n",
    "                z = np.dot(x, self.rff_weights)\n",
    "                return np.cos(z)\n",
    "            self.feature_map=feature_map\n",
    "        else:\n",
    "            def feature_map(x, nout):\n",
    "                sampler = RBFSampler(gamma=self.gamma, n_components=nout, random_state=RANDOM_STATE)\n",
    "                return sampler.fit_transform(x)\n",
    "            self.feature_map=feature_map\n",
    "\n",
    "    def __call__(self, x):\n",
    "        K = sklearn.metrics.pairwise.rbf_kernel(x, gamma=self.gamma)\n",
    "        return K\n",
    "\n",
    "\n",
    "class LinearKernel():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, z):\n",
    "        if len(z.shape) == 1:\n",
    "            z = z.reshape(z.shape[0], 1).copy()\n",
    "        return np.dot(z, z.T)\n",
    "    \n",
    "    def feature_map(self, z):\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NIK_Layer():\n",
    "    def __init__(self, nin, nout, sigma, kernel_x=RBFKernel, kernel_y=LinearKernel) -> None:\n",
    "        self.weights = None\n",
    "        self.kernel_x = kernel_x(nin, nout, sigma)\n",
    "        self.kernel_y = kernel_y()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ## Depending how the weights get calculated, the output dimension of X varies. Right now, it will be the amount of labels\n",
    "        ## This would not make much sense as it introduces massive bottlenecks in each layer... it is not clear how this works...\n",
    "\n",
    "        z = np.dot(x, self.weights.T)\n",
    "        return self.kernel_x.feature_map(z)\n",
    "\n",
    "    def print_kernel(self):\n",
    "        pass\n",
    "\n",
    "    def calculate_HSIC(self, x, y):\n",
    "        # for label in np.unique(y):\n",
    "        #     l_indices = np.where(y==label)\n",
    "        x = np.dot(x, self.weights.T)\n",
    "        return HSIC2(x, y, self.sigma, self.kernel_x, self.kernel_y)\n",
    "\n",
    "    def calculate_Ws(self, x, y):\n",
    "        # Future Note: If more closed form solutions for the weights are found for other kernels; implement this function as part of the kernel instead of the layer.\n",
    "\n",
    "        # Note: No normalization yet\n",
    "        # Note: output dimensions of Ws are (number of unique labels, number of features of X)\n",
    "        labels = np.unique(y)\n",
    "        Ws = np.zeros((labels.shape[0], x.shape[1]))\n",
    "        for i, label in enumerate(labels):\n",
    "            l_indices = np.where(y==label)\n",
    "            Ws[i,:] = np.sum(x[l_indices, :], axis=1)\n",
    "        self.weights = Ws\n",
    "\n",
    "    def calculate_Wopt(self):\n",
    "        raise ValueError(\"W* is not yet implemented\")\n",
    "\n",
    "\n",
    "class NIK_model():\n",
    "\n",
    "    def __init__(self, max_layer, n_features = 200, kernel_x=RBFKernel, kernel_y=LinearKernel, method=\"Ws\") -> None:\n",
    "        self.layers = []\n",
    "        self.max_layer = max_layer\n",
    "        self.method = method\n",
    "        self.kernel_x = kernel_x\n",
    "        self.kernel_y = kernel_y\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def fit(self, X: np.array, Y: np.array, verbose: bool = True, sigma_range = [0.00001, 0.001, 0.005, 0.01, 0.05, 0.1]):\n",
    "        self.unique_labels = np.unique(Y)\n",
    "        i = 1\n",
    "        while len(self.layers) < self.max_layer:\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Training layer {i}\")\n",
    "                print(\"------------------\")\n",
    "                i += 1\n",
    "\n",
    "            nin = X.shape[1]\n",
    "            current_layer = self.get_optimal_layer(X, Y, nin, sigma_range, verbose=verbose)\n",
    "\n",
    "            # Should be build to check if the layer improves the model. Currently not implemented so final amount of layers = self.max_layers\n",
    "            stopping_criteria = False\n",
    "            if stopping_criteria:\n",
    "                break\n",
    "\n",
    "            X = current_layer.forward(X)\n",
    "\n",
    "            self.layers.append(current_layer)\n",
    "\n",
    "            # if verbose:\n",
    "            #     accuracy()\n",
    "            \n",
    "\n",
    "    def get_optimal_layer(self, x, y, nin, sigma_range, verbose):\n",
    "        print(x.shape)\n",
    "        print(np.mean(x))\n",
    "        HSIC_optimal = 0\n",
    "        best_layer = None\n",
    "        nin = np.unique(y).shape[0]\n",
    "        for sigma in sigma_range:\n",
    "            candidate_layer = NIK_Layer(nin, self.n_features, sigma, kernel_x=self.kernel_x, kernel_y=self.kernel_y)\n",
    "            if self.method == \"Ws\":\n",
    "                candidate_layer.calculate_Ws(x, y)\n",
    "            elif self.method == \"W*\":\n",
    "                candidate_layer.calculate_Wopt()\n",
    "            else:\n",
    "                raise ValueError(f\"Method should either be 'Ws' or 'W*'. Got {self.method} instead\")\n",
    "\n",
    "            # This implementation of the HSIC might well be very wrong... it is just conflicting in every sense with the paper and his implementation\n",
    "            HSIC_candidate_layer = candidate_layer.calculate_HSIC(x, y)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Sigma: {sigma} - HSIC_candidate: {HSIC_candidate_layer} - HSIC_optimal: {HSIC_optimal}\")\n",
    "\n",
    "            if HSIC_candidate_layer > HSIC_optimal:\n",
    "                HSIC_optimal = HSIC_candidate_layer\n",
    "                best_layer = candidate_layer\n",
    "    \n",
    "        return best_layer\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 4, 4, 4],\n",
       "       [0, 0, 0, ..., 4, 4, 4],\n",
       "       [0, 0, 0, ..., 4, 4, 4]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(y.reshape(150, 1), y.reshape(150,1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HSIC2(x, y, sigma, KernelX, KernelY):\n",
    "    Kx = KernelX(x)\n",
    "    Ky = KernelY(y)\n",
    "\n",
    "    HKx = Kx - np.mean(Kx, axis=0) # equivalent to\t\tHKᵪ = H.dot(Kᵪ)\n",
    "    HKy = Ky - np.mean(Ky, axis=0) # equivalent to\t\tHKᵧ = H.dot(Kᵧ)\n",
    "\n",
    "    Hxy = np.trace(HKx.T*HKy)\n",
    "\n",
    "    # Hx = np.linalg.norm(HKx)\n",
    "    # Hy = np.linalg.norm(HKy)\n",
    "\n",
    "    # print(f\"Hxy: {Hxy}\")\n",
    "    # print(f\"HKy: {HKy}\")\n",
    "    # print(f\"Hx: {Hx}\")\n",
    "    # print(f\"Hy: {Hy}\")\n",
    "\n",
    "    # hsic = Hxy / (Hx * Hy)\n",
    "\n",
    "    return Hxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = NIK_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(150,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer 1\n",
      "------------------\n",
      "(150, 4)\n",
      "3.4644999999999997\n",
      "Sigma: 1e-05 - HSIC_candidate: 99.33333333333331 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 99.30676582293367 - HSIC_optimal: 99.33333333333331\n",
      "Sigma: 0.005 - HSIC_candidate: 99.30667064001359 - HSIC_optimal: 99.33333333333331\n",
      "Sigma: 0.01 - HSIC_candidate: 99.30666766005889 - HSIC_optimal: 99.33333333333331\n",
      "Sigma: 0.05 - HSIC_candidate: 99.30666670640305 - HSIC_optimal: 99.33333333333331\n",
      "Sigma: 0.1 - HSIC_candidate: 99.30666667660076 - HSIC_optimal: 99.33333333333331\n",
      "Training layer 2\n",
      "------------------\n",
      "(150, 200)\n",
      "0.9984815282802902\n",
      "Sigma: 1e-05 - HSIC_candidate: 99.30666666666666 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 99.28045225782088 - HSIC_optimal: 99.30666666666666\n",
      "Sigma: 0.005 - HSIC_candidate: 99.26091869666232 - HSIC_optimal: 99.30666666666666\n",
      "Sigma: 0.01 - HSIC_candidate: 99.21621285347086 - HSIC_optimal: 99.30666666666666\n",
      "Sigma: 0.05 - HSIC_candidate: 98.94643714967114 - HSIC_optimal: 99.30666666666666\n",
      "Sigma: 0.1 - HSIC_candidate: 98.65467402392095 - HSIC_optimal: 99.30666666666666\n",
      "Training layer 3\n",
      "------------------\n",
      "(150, 200)\n",
      "0.9863057125104685\n",
      "Sigma: 1e-05 - HSIC_candidate: 99.27999999999999 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 99.01809082839824 - HSIC_optimal: 99.27999999999999\n",
      "Sigma: 0.005 - HSIC_candidate: 98.13605107085102 - HSIC_optimal: 99.27999999999999\n",
      "Sigma: 0.01 - HSIC_candidate: 97.0461115620473 - HSIC_optimal: 99.27999999999999\n",
      "Sigma: 0.05 - HSIC_candidate: 88.55139722049955 - HSIC_optimal: 99.27999999999999\n",
      "Sigma: 0.1 - HSIC_candidate: 78.30280233917226 - HSIC_optimal: 99.27999999999999\n",
      "Training layer 4\n",
      "------------------\n",
      "(150, 200)\n",
      "0.9855318699258896\n",
      "Sigma: 1e-05 - HSIC_candidate: 98.33333333333331 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 91.64278602969071 - HSIC_optimal: 98.33333333333331\n",
      "Sigma: 0.005 - HSIC_candidate: 64.31764792432357 - HSIC_optimal: 98.33333333333331\n",
      "Sigma: 0.01 - HSIC_candidate: 38.37010715244694 - HSIC_optimal: 98.33333333333331\n",
      "Sigma: 0.05 - HSIC_candidate: 2.7597605863322885 - HSIC_optimal: 98.33333333333331\n",
      "Sigma: 0.1 - HSIC_candidate: 0.707099274403787 - HSIC_optimal: 98.33333333333331\n",
      "Training layer 5\n",
      "------------------\n",
      "(150, 200)\n",
      "0.9867164192651501\n",
      "Sigma: 1e-05 - HSIC_candidate: 59.69333333333334 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 4.407314409901954 - HSIC_optimal: 59.69333333333334\n",
      "Sigma: 0.005 - HSIC_candidate: 0.18636652240460916 - HSIC_optimal: 59.69333333333334\n",
      "Sigma: 0.01 - HSIC_candidate: 0.046674532657432266 - HSIC_optimal: 59.69333333333334\n",
      "Sigma: 0.05 - HSIC_candidate: 0.0018680447079266749 - HSIC_optimal: 59.69333333333334\n",
      "Sigma: 0.1 - HSIC_candidate: 0.00046701948932392767 - HSIC_optimal: 59.69333333333334\n",
      "Training layer 6\n",
      "------------------\n",
      "(150, 200)\n",
      "0.9862584603846881\n",
      "Sigma: 1e-05 - HSIC_candidate: 28.37333333333333 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 0.9459769602049932 - HSIC_optimal: 28.37333333333333\n",
      "Sigma: 0.005 - HSIC_candidate: 0.03851575107137095 - HSIC_optimal: 28.37333333333333\n",
      "Sigma: 0.01 - HSIC_candidate: 0.009634296878948234 - HSIC_optimal: 28.37333333333333\n",
      "Sigma: 0.05 - HSIC_candidate: 0.00038544050937883156 - HSIC_optimal: 28.37333333333333\n",
      "Sigma: 0.1 - HSIC_candidate: 9.636066372498142e-05 - HSIC_optimal: 28.37333333333333\n",
      "Training layer 7\n",
      "------------------\n",
      "(150, 200)\n",
      "0.9845042029566696\n",
      "Sigma: 1e-05 - HSIC_candidate: 22.813333333333333 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 0.7405401720121261 - HSIC_optimal: 22.813333333333333\n",
      "Sigma: 0.005 - HSIC_candidate: 0.03013051802177502 - HSIC_optimal: 22.813333333333333\n",
      "Sigma: 0.01 - HSIC_candidate: 0.0075366574018558286 - HSIC_optimal: 22.813333333333333\n",
      "Sigma: 0.05 - HSIC_candidate: 0.00030151788007737323 - HSIC_optimal: 22.813333333333333\n",
      "Sigma: 0.1 - HSIC_candidate: 7.537987314876204e-05 - HSIC_optimal: 22.813333333333333\n",
      "Training layer 8\n",
      "------------------\n",
      "(150, 200)\n",
      "0.9869315476536529\n",
      "Sigma: 1e-05 - HSIC_candidate: 23.199999999999992 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 0.7389735117592773 - HSIC_optimal: 23.199999999999992\n",
      "Sigma: 0.005 - HSIC_candidate: 0.030051735970746174 - HSIC_optimal: 23.199999999999992\n",
      "Sigma: 0.01 - HSIC_candidate: 0.007516832399320217 - HSIC_optimal: 23.199999999999992\n",
      "Sigma: 0.05 - HSIC_candidate: 0.0003007232206384547 - HSIC_optimal: 23.199999999999992\n",
      "Sigma: 0.1 - HSIC_candidate: 7.518119532856637e-05 - HSIC_optimal: 23.199999999999992\n",
      "Training layer 9\n",
      "------------------\n",
      "(150, 200)\n",
      "0.9849633785735546\n",
      "Sigma: 1e-05 - HSIC_candidate: 22.46666666666667 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 0.7098408097567814 - HSIC_optimal: 22.46666666666667\n",
      "Sigma: 0.005 - HSIC_candidate: 0.028860731840395504 - HSIC_optimal: 22.46666666666667\n",
      "Sigma: 0.01 - HSIC_candidate: 0.007218877257650025 - HSIC_optimal: 22.46666666666667\n",
      "Sigma: 0.05 - HSIC_candidate: 0.00028880240063289264 - HSIC_optimal: 22.46666666666667\n",
      "Sigma: 0.1 - HSIC_candidate: 7.220096990323555e-05 - HSIC_optimal: 22.46666666666667\n",
      "Training layer 10\n",
      "------------------\n",
      "(150, 200)\n",
      "0.986839334906418\n",
      "Sigma: 1e-05 - HSIC_candidate: 2.559999999999995 - HSIC_optimal: 0\n",
      "Sigma: 0.001 - HSIC_candidate: 0.0751682870618069 - HSIC_optimal: 2.559999999999995\n",
      "Sigma: 0.005 - HSIC_candidate: 0.003049939545686442 - HSIC_optimal: 2.559999999999995\n",
      "Sigma: 0.01 - HSIC_candidate: 0.0007628257775849079 - HSIC_optimal: 2.559999999999995\n",
      "Sigma: 0.05 - HSIC_candidate: 3.0517396226059645e-05 - HSIC_optimal: 2.559999999999995\n",
      "Sigma: 0.1 - HSIC_candidate: 7.629383162566228e-06 - HSIC_optimal: 2.559999999999995\n"
     ]
    }
   ],
   "source": [
    "c.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 200)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.predict(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "\n",
    "y_pred = clf.fit(c.predict(X), y).predict(c.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7466666666666667"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred=y_pred, y_true=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d37477634065a47ffc8c6f154c95428d989ebe7437b6057a428faa88f1323bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
